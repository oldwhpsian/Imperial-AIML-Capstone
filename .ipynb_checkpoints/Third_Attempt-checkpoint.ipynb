{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "287e1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes_functions import (BlackBoxFunction, import_weekly_queries_results, \n",
    "    save_all_data, backup_all_data, make_mesh)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b34ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_1 = BlackBoxFunction('function_1')\n",
    "function_2 = BlackBoxFunction('function_2')\n",
    "function_3 = BlackBoxFunction('function_3')\n",
    "function_4 = BlackBoxFunction('function_4')\n",
    "function_5 = BlackBoxFunction('function_5')\n",
    "function_6 = BlackBoxFunction('function_6')\n",
    "function_7 = BlackBoxFunction('function_7')\n",
    "function_8 = BlackBoxFunction('function_8')\n",
    "\n",
    "all_bbox_functions = [function_1, function_2, function_3, function_4, function_5, function_6, function_7, function_8]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c9bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries, results = import_weekly_queries_results(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14478c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ''' Loop that will add the weeks queries to the data in the function objects then save the data'''\n",
    "# # ''' Only run this cell once per week.'''\n",
    "# for query, result, function in zip(queries, results, all_bbox_functions):\n",
    "#     function.add_query(query, result)\n",
    "#     function.export_data_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9ace84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01, 0.01]\n",
      "Done\n",
      "[0.01, 0.01]\n",
      "Done\n",
      "[0.01, 0.01, 0.01]\n",
      "Done\n",
      "[0.01, 0.01, 0.01, 0.01]\n",
      "Done\n",
      "[0.01, 0.01, 0.01, 0.01]\n",
      "Done\n",
      "[0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "Done\n",
      "[0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "Done\n",
      "[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "real_noise_std = 1e-10 \n",
    "noise_assumption = 1e-10\n",
    "mesh_counts = {1: 1000, 2: 1000, 3: 200, 4: 50, 5: 12, 6: 10, 7: 6, 8: 6}\n",
    "beta = 7\n",
    "next_queries_UCB = []\n",
    "next_queries_PI = []\n",
    "for i, function in enumerate(all_bbox_functions):\n",
    "    rbf_lengthscale = [0.01 for d in range(function.io[0])]\n",
    "    print(rbf_lengthscale)\n",
    "    kernel = RBF(length_scale=rbf_lengthscale, length_scale_bounds='fixed')\n",
    "    model2 = GaussianProcessRegressor(kernel = kernel, alpha=noise_assumption)\n",
    "    model2.fit(np.array(function.X), np.array(function.Y))\n",
    "    working_mesh = make_mesh(function.io[0], 0, 0.999999, mesh_counts[function.io[0]])\n",
    "    working_mesh = pd.DataFrame(working_mesh, columns = function.X.columns)\n",
    "    post_mean, post_std = model2.predict(np.array(working_mesh), return_std=True)\n",
    "    \n",
    "    # UPPER CONFIDENCE BOUND\n",
    "    UCB = post_mean + beta*post_std\n",
    "    \n",
    "    # PROBABILITY OF IMPROVEMENT\n",
    "    PI = norm.cdf((post_mean-max(function.Y)/post_std))\n",
    "    \n",
    "    # idx_max\n",
    "    idx_max_UCB = np.argmax(UCB)\n",
    "    idx_max_PI = np.argmax(PI)\n",
    "    \n",
    "    next_query_UCB = working_mesh.iloc[idx_max_UCB]\n",
    "    next_query_PI = working_mesh.iloc[idx_max_PI]\n",
    "    next_queries_UCB.append(next_query_UCB.tolist())\n",
    "    next_queries_PI.append(next_query_PI.tolist())\n",
    "    \n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72406bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.594594, 0.624624], [0.711711, 0.35135099999999997], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.444444, 0.333333, 0.0, 0.888888, 0.999999, 0.111111], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(next_queries_PI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b96c189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.594594, 0.624624],\n",
       " [0.711711, 0.351351],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.444444, 0.333333, 0.0, 0.888888, 0.999999, 0.111111],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collecting the predicitions for each acquisition function into lists\n",
    "# Each list is a list of lists holding the next queries (inputs) for each function\n",
    "UCB_preds = [[round(j,6) for j in i] for i in next_queries_UCB]\n",
    "PI_preds = [[round(j,6) for j in i] for i in next_queries_PI]\n",
    "PI_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04aaaf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCB:  [False, False, False, False, False, False, False, True] \n",
      "PI:   [False, False, True, True, True, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "# This block of code checks to see if the queries generated for each function above have been previously queried\n",
    "UCB_present = []\n",
    "PI_present = []\n",
    "\n",
    "for pred_UCB, pred_PI, function in zip(UCB_preds, PI_preds, all_bbox_functions):\n",
    "    UCB_present.append((function.X == np.array(pred_UCB)).all(1).any())\n",
    "    PI_present.append((function.X == np.array(pred_PI)).all(1).any())\n",
    "print('UCB: ', UCB_present, '\\nPI:  ', PI_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bcfac51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.596596-0.644644\n",
      "0.686686-0.910910\n",
      "0.000000-0.000000-0.085427\n",
      "0.000000-0.000000-0.000000-0.102041\n",
      "0.306122-0.938775-0.959183-0.877550\n",
      "0.000000-0.000000-0.000000-0.000000-0.090909\n",
      "0.444444-0.333333-0.000000-0.888888-0.999999-0.111111\n",
      "0.200000-0.400000-0.200000-0.799999-0.599999-0.799999-0.400000-0.799999\n"
     ]
    }
   ],
   "source": [
    "# This block of code simply formats the queries so they can be copy/pasted into the google doc\n",
    "for lst in UCB_preds:\n",
    "    output = []\n",
    "    for number in lst:\n",
    "        num_str = str(number)\n",
    "        wl = num_str.split('.')\n",
    "        if len(wl[1])<6:\n",
    "            wl[1] = wl[1] + '0'*(6-len(wl[1]))\n",
    "            num_str = '.'.join(wl)\n",
    "        output.append(num_str)\n",
    "    print('-'.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b05356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_5.data.iloc[-9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b479c3",
   "metadata": {},
   "source": [
    "How did this whole thing evolve? Very simply at first. I didn't really know what I was doing on the ML algorithm side of things so I first wrote code to sweep through the inputs in each dimension and find the largest interval in that dimension that had not been sampled. I then cut that interval in half and sampled that point. repeat for each dimension.\n",
    "\n",
    "Then I wrote some code that actually performed BO using a class called BlackBoxFunction that imports and holds all the data for each function, and has methods to call for the data etc. I combined this with a BO algorithm using an RBF kernel and GaussianProcessRegressor. The main thing that I found challenging was the generation of the sample space in n-dimensions. i.e. for 1D you can simply call linspace(0,0.9999, 10000) to have an extremely densely packed space from which to sample (i.e. over which to build the surrogate. I created a function called make_mesh that took in the number of dimensions and required \"density\" of the space in order to generate n-dimensional meshes which I could use to generate the surrogate model. I quickly learned the difficulties of expanding to higher dimensions, as my make_mesh function relied on nested for loops that had extremely poor time complexity of O(n^D). As the dimensions D increased, the function became exponentially less efficient to compute the spaces. Thus, as the functions grew in dimension I had to severly limit the density at which each dimension could be sampled. As an example, for 2D functions I can get pack each of the dimensions with 1000 points, but this had to drop to 200 points per dimension in 3D, and down to 6 (!) points per dimension in 8D. I currently still sit with this problem, that I can truly not even begin to properly explore the space! I have thought of perhaps using another function to generate these n-dimensional meshes, analagous to the MATLAB function MESHGRID or NGRID. Indeed numpy itself has a function called meshgrid that I stumbled upon when searching for the MATLAB equivalent. Unfortunately, numpy's meshgrid seemed to suffer from the same curse of dimensionality.\n",
    "\n",
    "Initially I only calculated UCB for a couple weeks (after I built the above, ofc). Then added PI calculations as well such that I could compare the two manually to see the differences. UCB seemed to be very exploitative regardless of the value for B I gave it. Both models gave a problem that they would sometimes recommend the same points that have been previously queried! To help combat this I wrote some code to check if the new queries generated already existed in the data, such that I may know that I needed to do something to recify the issue.\n",
    "\n",
    "To help combat the repeated queries of the same points, I went back to the bayesian optimization notebook from week 12. I ran tests on known functions and identified that the model was again converging onto the same point repeatedly after just 13 iterations. This was happening on input intervals of [0,1] but not on larger intervals. In fact, the larger the interval the more iterations it took for the model to converge. This tipped me off to the length scale of the kernel. Having a tight input domain of [0,1] means that the length scale will have a larger effect. Reducing the length scale helped allieviate my problem of seeing the same repeated queries, though the problem was still there in a big way for the PI calculations. \n",
    "\n",
    "- Next up I will look into the possibility of using other kernels for the GP model\n",
    "- I will look deeper into the length scale to see if there are general heuristics that can help choose the scale, ideally in an automated way that will vary depending on the size of the dataset/the number of weeks/epochs that have passed. \n",
    "\n",
    "\n",
    "### I think I identified a workflow error! I would load the previous queries and data in and have it exported to excel. But for some reason, that data was not being incorporated into function.data in that process. That meant that when making the predicitions, the model was just predicting the same values as the previus week as it thought that it hadn't made that prediction yet!\n",
    "\n",
    "### Function 5 predicted a big fish on interation 47! It will likely want to stay stuck there now since the others are so much smaller. Maybe I need to generate local meshes around optima to deter the mdoels from getting stuck in higher dimensions where I have less dense dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd805f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0db83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8ff56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c1209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44fedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245056d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6562c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = np.linspace(0,99,100)\n",
    "xx, yy = np.meshgrid(space, space, sparse=True)\n",
    "zz = xx**2 + yy**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xx, yy, zz)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f50f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
